[
["index.html", "Einführung in die Datenauswertung mit R Prolog 0.1 Vorbereitung auf den Kurs 0.2 Aufbau des Kurses 0.3 Inhalt des Kurses 0.4 Resourcen 0.5 Sonstige Quellen", " Einführung in die Datenauswertung mit R Jannik Buhr 2020-11-22 Prolog 0.1 Vorbereitung auf den Kurs Der Kurs wird zwar auf Deutsch stattfinden, aber ich kann nicht garantieren, dass nicht einige Konzepte in diesem Buch dennoch auf Englisch erklärt werden, da die verwendeten Fachbegriffe es ohnehin schon bereits sind. Eine Übersetzung würde daher nur mehr Verwirrung stiften. Zudem findet sich auf Englisch die meiste Lektüre und das Googeln von Fehlermeldungen wird erheblich erleichtert, da mehr Leute im Internet auf Englisch unterwegs sind als auf Deutsch. Hier findet ihr Links zu R und RStudio, damit ihr beides bereits auf euren Laptops oder Computern installieren könnt. Persönlich empfehle ich das Arbeiten am eigenen Gerät, aber für alle, die keine Laptops besitzen oder mitbringen möchten, werden Computer (Mac) zur Verfügung stehen. R: https://cran.r-project.org/ RStudio: https://www.rstudio.com/products/rstudio/download/#download Für die Verwendung der vorhandenen statt eigener Computer würde ich euch bitten, euch unter folgendem Link bei RStudio Cloud anzumelden. Dabei handelt es sich um eine Version von RStudio, die auf einem Server läuft und von euch über jeden beliebigen Browser aufgerufen werden kann. Auf diese Weise sind wir nicht von der IT-Abteilung des ZMBHs abhängig, die neuste Version von R und RStudio zu installieren. Zusätzlich könnt ihr auch von zu Hause auf eure Dateien zugreifen. RStudio Cloud Dort müsst ihr euch dann nur anmelden und weiter nichts tun, die neusten Versionen von R und RStudio sind dort bereits installiert. Bei Installationsschwierigkeiten meldet euch möglichst bereits vor Kursbeginn. 0.2 Aufbau des Kurses Vom 18. Oktober bis 29. November (6 Freitage) Morgen-Session (mehr Theorie) 10 Uhr (ct) bis 12 Uhr Nachmittags-Session (mehr Praxis) 13 Uhr bis … (maximal 17 Uhr) 0.3 Inhalt des Kurses Einleitung Was ist dieses R? R und RStudio R als Taschenrechner .R-Dateien (Skripte) Variablen und arithmetische Operationen Wir machen es uns gemütlich in RStudio Einstellungen, Themes, etc. Project-based Workflow R Markdown Das Tidyverse (und andere Packages) Hilfe finden Die Community StackOverflow, GitHub, R4DS, Slack, Advanced R Arten von Daten Daten in der Wildnis Data in R Vector, matrix, array, list, data.frame (tibble) Data formats, Getting data into R Mein erster Plot Das letzte Kuchendiagramm Barplots Base R vs ggplot2 The grammar of graphics Scatterplots Tidy data Prinzip Daten importieren Data-Wrangling mit dplyr and tidyr Funktionale Programmierung (vs OOP) Funktionen schreiben FP vs. OOP Pure functions und Functional Programming Statistik Basics: sd, var, mean, median, correlation Histogramme, Verteilungen p-values t.test, Wilcoxon rank sum test, quisquared (ANOVA) Modelling and data fitting Lineare Regression Analyse modelr, broom \\(R^2\\), rmse, residuals, plots, non-linear regression Many models nested datframes, list colums map Funktionen Diese Inhaltsangabe ist thematisch, nicht chronologisch sortiert und erhebt keinen Anspruch auf Vollständigkeit. Alle Angaben ohne Gewähr. Bei Risiken und Nebenwirkungen fragen Sie Ihren Arzt oder Apotheker. 0.4 Resourcen 0.4.1 Tidyverse R for Data Science (Wickham und Grolemund 2017) R4DS online Community RStudio Cheat Sheets! The Modern Dive (Kim 2019) RStudio Education 0.4.2 Allgemein R Advanced R (Wickham 2019) Hands on Programming with R (Grolemund und Wickham 2014) R Packages (Wickham 2015) Data Visualization: A Practical Introduction (Healy 2018) Graph Cookbook (Chang 2013) 0.4.3 Statistic Intuitive Biostatistics (Motulsky 2017) Statistics Done Wrong (Reinhart 2015) 0.4.4 Talks, Podcasts, Blogs David Robinson, YouTube [] 0.4.5 Misc Unglaublich niedliche Illustrationen (Horst 2019) Happy Git with R 0.5 Sonstige Quellen Tidytuesday Tips for Working with Images in Rmarkdown Made with the help of these amazing packages (plus documentation): R Core Team (2019); Xie (2019a); Xie (2019b); Allaire u. a. (2019); Xie (2015) "],
["day1.html", "Tag 1 Los Geht’s 1.1 Grundlegend Strukturen in R 1.2 Project-based Workflow 1.3 Packages: Das Tidyverse", " Tag 1 Los Geht’s 1.1 Grundlegend Strukturen in R Da R eine Programmiersprache ist befassen wir uns zunächst mit der grundlegenden Syntax sowie den basis-Datenstrukturen, in denen wir Daten in R vorfinden können. Öffne ein R-Script (eine Textdatei mit der Endung .R; In RStudio der Button oben links, oder der Shortcut Ctrl+Shift+N)1. Nun kannst du die folgenden Befehle selbst ausprobieren, indem du sie in das Script schreibst (oder kopierst) und mit dem Shortcut Ctrl+Enter in die Konsole schickst. Dort wird dann die ausgewählte Zeile / der markierte Code ausgeführt und das Ergebnis angezeigt. Inspiriert wurde diese Zusammenfassung von learnXinYminutes. Die aufgeführten Konzept liegen der Funktionsweise von R zu Grunde, dennoch ist tieferes Verständnis nicht zwingend notwendig, um effektiv mit Werkzeugen wie den Packages des tidyverse zu arbeiten. Daher: nicht verzweifeln, wenn unweigerlich etwas nicht funktioniert! Das ist völlig normal. Selbst erfahrene Programmierer stolpern regelmäßig über fehlende Kommata oder Klammern. Abb. 1.1: The most important programming advice. 1.1.1 Rechnen # Kommentare starten mit dem #-Symbol. # Diese Zeilen tun also nichts. # Aber erinnern Dein zukünftiges Ich daran, was du eigentlich vor hattest Zahlen in R können als ganze Zahlen (integer), rationale Zahlen (numeric oder auch double) oder, wenn auch selten verwendet, als komplexe Zahlen (complex) vorliegen. Zusätzlich gibt es noch text (character) und Wahr/Falsch (boolean oder auch logical) sowie kategorische Daten (factor), NULL und Not Assigned (NA). # Integer (gekennzeichnet mit L hinter der Zahl) 5L # Numeric 5 5.3 # Complex 3 + 4i # Character (doppelte oder einfache Anführungszeichen sind erlaubt) &quot;Horatio&quot; &#39;Horatio&#39; # logical TRUE | FALSE # factor # The factor class is for categorical data # Factors can be ordered (like grade levels) or unordered (like gender) factor(c(&quot;f&quot;, &quot;m&quot;, &quot;m&quot;, NA, &quot;f&quot;, &quot;nb&quot;)) # NULL NULL Werte können Variablen zugewiesen werden, um später Zugriff darauf zu haben. # Namen in R dürfen Buchstaben und Zahlen sowie _ enthalten. # Großschreibung macht einen Unterschied avogadro &lt;- 6.02e23 # Neben der normalen Art der Zuweisung ( mit &lt;- ) # funktioniert auch Folgendes eulers_number = 2.718282 &quot;Harry Potter&quot; -&gt; main_character Den Inhalt der Variable erhältst du, indem du nur den die Variable in der Konsole ausführst oder mit Ctrl+Enter in die Konsole schickst: # Implizit ist es eine Abkürzung für print(main_character) main_character ## [1] &quot;Harry Potter&quot; Grundlegende Rechenfunktionen: 10L + 66L 53.2 - 4 2.0 * 2L 3L / 4L 5 %/% 2 # Modulo 5 %% 2 # Rest 0 / 0 # NaN = Not a Number 1.1.2 Funktionen und Vektoren Funktionen sind mit das Wichtigste in einer Programmiersprache. Funktionen in R erkennst du an den Klammern, zwischen die die Argumente/Parameter der Funktion gehören. Gemeint sind nicht nur mathematische Funktionen, alles, das etwas tut, ist eine Funktion. sin(pi / 2) ## [1] 1 ?funktion öffnet die Hilfe zu dieser Funktion (in RStudio rechts unten) und ist damit der wohl wichtigste Befehl überhaupt. Noch schneller ist der Shortcut F1, wenn der Cursor über einer Funktion ist. Niemand merkt sich alle Befehle, also nutzte die Hilfe reichlich! ?sin Alle Werte in R, selbst einzelne, sind Vektoren, also eine Reihe Dinge der gleichen Art. Die meisten Operationen in R arbeiten automatisch elementeweise mit Vektoren. # c steht für combine oder concatenate vector1 &lt;- c(3, 3, 3, 2, 2) vector2 &lt;- vector1 + vector1 vector2 ## [1] 6 6 6 4 4 vector1 * 2 ## [1] 6 6 6 4 4 Der : Operator erstellt einen Vektor mit den Zahlen von a bis b für a:b. Mehr Kontrolle bietet die Funktion seq für sequence. viele_zahlen &lt;- 1:1000 gerade_zahlen &lt;- seq(from = 2, to = 1000, by = 2) Vergleiche zwischen Elementen/Dingen: TRUE == FALSE # Vergleich, Ergebnis ist FALSE TRUE != FALSE # Ungleich, TRUE 1 &lt; 3 # TRUE FALSE | TRUE # Oder FALSE &amp; TRUE # Und # Applying | and &amp; to vectors returns elementwise logic operations c(TRUE,FALSE,FALSE) | c(FALSE,TRUE,FALSE) Hier noch einmal die Erinnerung, dass eine Fehlermeldung kein Weltuntergang ist. In diesem Code ist beispielsweise ein Komma zu viel: c(1,3,) ## Error in c(1, 3, ): argument 3 is empty Im Zweifel gilt immer der Rat aus Abbildung 1.1. Bei der Kombination von unterschiedlichen Datentypen wird automatisch der gewählt, der mehr zulässt. c(1.4, 5L) ## [1] 1.4 5.0 c(12, &quot;text&quot;) ## [1] &quot;12&quot; &quot;text&quot; # Oder explizit: as.character(12) ## [1] &quot;12&quot; as.integer(&quot;12&quot;) ## [1] 12 Du kannst deine eigenen Funktionen definieren: ## Everything that does something is a function, everything that exists is an object greet &lt;- function(name) { paste(&quot;Good morning&quot;, name) } greet(&quot;Jannik&quot;) ## [1] &quot;Good morning Jannik&quot; 1.1.3 Datenstrukturen 1D, gleicher Datentype: Vector vec &lt;- c(8, 9, 10, 11) # We ask for specific elements by subsetting with square brackets [] # (Note that R starts counting from 1) vec[1] letters[18] letters[1:3] # Subsetting mit einem logical vector selectThis &lt;- vec %% 2 == 0 vec[selectThis] # Oder mit den Indices vec[which(vec %% 2 == 0)] Praktische Funtionen für Vektoren: # grab just the first or last few entries in the vector, head(vec) tail(vec, 2) # or figure out if a certain value is in the vector any(vec == 10) # If an index &quot;goes over&quot; you&#39;ll get NA: vec[6] # You can find the length of your vector with length() length(vec) # You can perform operations on entire vectors or subsets of vectors vec = vec * 4 vec[2:3] * 5 any(vec[2:3] == 8) # and R has many built-in functions to summarize vectors mean(vec) median(vec) var(vec) sd(vec) max(vec) min(vec) sum(vec) 1D, beliebige Datentypen: List meine_liste &lt;- list(12, &#39;dog&#39;, 1:10) # Einfache [] geben eine Teilliste zurück meine_liste[2] # Doppelte [[]] geben das Element der List (entpackt aus der Liste) zurück meine_liste[[2]] # Listen und Vektoren können Namen haben names(meine_liste) &lt;- c(&quot;zahl&quot;, &quot;hund&quot;, &quot;vector&quot;) # Die Namen identifizieren die Elemente und $ gibt das entsprechende Element zurück meine_liste$zahl meine_liste$vector 2D, gleicher Datentyp: matrix # You can make a matrix out of entries all of the same type like so: mat &lt;- matrix(nrow = 3, c(1,2,3,4,5,6), byrow = TRUE ) myMatrix &lt;- matrix(ncol = 2, c(1,2,3,4)) # Ask for the first row mat[1:2, ] # Perform operation on the first column 3 * mat[, 1] # Ask for a specific cell mat[3,2] # Transpose the whole matrix t(mat) # Matrix multiplication mat %*% t(mat) # cbind() sticks vectors together column-wise to make a matrix mat2 &lt;- cbind(1:4, c(&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;dog&quot;)) # Because matrices must contain entries all of the same class, # everything got converted to the character class # rbind() sticks vectors together row-wise to make a matrix mat3 &lt;- rbind(c(1,2,4,5), c(6,7,0,4)) 2D, beliebige Datentypen: data.frame students &lt;- data.frame(c(&quot;Cedric&quot;,&quot;Fred&quot;,&quot;George&quot;,&quot;Cho&quot;,&quot;Draco&quot;,&quot;Ginny&quot;), c(3, 2, 2, 1, 0, -1), c(&quot;H&quot;, &quot;G&quot;, &quot;G&quot;, &quot;R&quot;, &quot;S&quot;, &quot;G&quot;), stringsAsFactors = FALSE) names(students) &lt;- c(&quot;name&quot;, &quot;year&quot;, &quot;house&quot;) # name the columns nrow(students) ncol(students) dim(students) # The data.frame() function converts character vectors to factor vectors # by default; turn this off by setting stringsAsFactors = FALSE when # you create the data.frame ?data.frame # There are many ways to subset data frames # See if you can find out what each of these lines does students$year students[ , -c(1,3)] students[, &quot;year&quot;] Eine fast so hilfreiche Funktion wie die Hilfe über ? ist die Funktion str für structure str(students) ## &#39;data.frame&#39;: 6 obs. of 3 variables: ## $ name : chr &quot;Cedric&quot; &quot;Fred&quot; &quot;George&quot; &quot;Cho&quot; ... ## $ year : num 3 2 2 1 0 -1 ## $ house: chr &quot;H&quot; &quot;G&quot; &quot;G&quot; &quot;R&quot; ... 1.1.4 Die ersten Plots Scatterplots x &lt;- 1:20 y &lt;- x + rnorm(n = length(x) ) plot(x = x, y = y, main = &quot;fake data&quot;) Mit linearer Regression model &lt;- lm(y ~ x) plot(x = x, y = y, main = &quot;Linear Fit&quot;) abline(model , col = &#39;red&#39; ) Kurven von Funktionen curve(sin, from = -5, to = 5) Histogramme hist(y) Balkendiagramme barplot(students$year + 2, names.arg = students$name) Mach es bunt! barplot(students$year + 2, names.arg = students$name, col = factor(students$house) ) Die Funktion par erlaubt das setzen von graphical parameters wie der Hintergrundfarbe oder den plot margins. Es wirkt sich auf alle Plots danach aus, bis die R session neu gestartet wird. Dies ist das einzige Kuchendiagramm, dass du jemals brauchen wirst. par(bg = &quot;pink&quot;) cols &lt;- c(&#39;#0292D8&#39;, &#39;#F7EA39&#39;, &#39;#C4B632&#39;) pie( c(280, 60, 20), init.angle = -50, col = cols, border = NA, labels = NA ) legend(1, 1, xjust = 0.5, yjust = 1, fill = cols, border = NA, legend = c(&#39;Sky&#39;, &#39;Sunny side of pyramid&#39;, &#39;Shady side of pyramid&#39;) ) Speichere deinen Plot indem du zunächst ein Graphic-Device öffnest (png in diesem Beispiel), dann den Plot erstellst und zum Schluss das Graphic-Device wieder ausschaltest um das “Drucken” abzuschließen. png(&#39;my_plot.png&#39;, width = 500, height = 400) # ... Plot code (siehe oben) hier dev.off() 1.2 Project-based Workflow Wichtige RStudio-Einstellungen: Abb. 1.2: RStudio Einstellungen 1.3 Packages: Das Tidyverse Packages sind Sammlungen von R-Funktionen anderer Nutzer, die dir unglaublich viel Arbeit abnehmen oder erleichtern. # Installation install.packages(&quot;tidyverse&quot;) # Paket laden library(tidyverse) Das tidyverse ist eine Gruppe von Packages, die gemeinsam installiert und geladen werden und sehr gut zusammespielen. Wir werden ausgiebig damit arbeiten. Für Mac-Nutzer wird Ctrl durch Cmd ersetzt↩︎ "],
["day2.html", "Tag 2 An die Daten 2.1 Wiederholung Tag 1 2.2 Workflow einer Datenauswertung 2.3 Daten einlesen 2.4 The pipe and dplyr verbs 2.5 Was ist Wahrscheinlichkeit? 2.6 Transfer auf neue Daten", " Tag 2 An die Daten 2.1 Wiederholung Tag 1 Was ergibt c(\"12\", 13, 14)[2] + 1 und warum? Erstelle einen Vektor x mit den ganzen Zahlen von 1 bis 10 Erstelle einen Vektor y, der x entspricht Plotte die beiden Vektoren gegeneinander als Punkte Lege eine lineare Regressionslinie durch die Daten und füge sie dem Plot hinzu Wie können wir mehr über das linear model-Objekt erfahren? # c(&quot;12&quot;, 13, 14)[2] + 1 x &lt;- 1:10 y &lt;- x plot(x, y) model &lt;- lm(y ~ x) abline(model, col = &quot;red&quot;) str(model) ?lm 2.2 Workflow einer Datenauswertung Abb. 2.1: Quelle: Wickham und Grolemund (2017) 2.2.1 Communication! Rmarkdown ermöglicht es uns, den Code zur Datenauswertung mitsamt den Ergebnissen, Plots und Gedanken dazu an einem Ort zu sammeln und zu dokumentieren. Im Kurs arbeiten wir ebenfalls mit Rmarkdown, da wir so wunderbar die zeigen können, was jede Zeile des Codes tut, da der Output direkt darunter angezeigt werden kann. In RStudio kannst du ein neues Rmarkdown Dokument oben links mit dem “new document” Button. Eine umfassende Anleitung zu Rmarkdown findest du im Rmarkdown guide, der natürlich selbst ebenfalls in Rmarkdown geschrieben ist (genau wie dieses Buch). Neue Code-Chunks fügst du mit Ctrl+Alt+I ein. Nutze sie reichlich um den Code in sinnvolle Teile zu strukturieren und nutze Text außerhalb der Chunks um die Gedankengänge bei der Datenauswertung festzuhalten, ähnlich wie dieses Dokument es tut.2 2.3 Daten einlesen Innerhalb des Tidyverse ist dazu das readr-Package verantwortlich. Die meisten Funktionen diese Packages beginnen mit read_ oder write_ und die Autovervollständigung zeigt dir alle Optionen auf. Zunächst laden wir das tidyverse: library(tidyverse) Ich habe diesem Codeblock noch die Option “```{r, message=FALSE} …” gegeben, damit die Begrüßungsnachricht des tidyverse nicht in unserem Bericht landet. In einem beliebigen Tabellen-Programm (Excel in unserem Fall) erstellen wir eine Liste mit den Kursteilnehmerinnen und Teilnehmern und halten zusätzlich fest, ob sie in lange (1) oder kurze (0) Haare habe und in welcher Reihe sie sitzen. Wir speichern die Datei im data Ordner unseres RStudio-Projekt-Ordners als “.csv”-Datei. # Read data with read_csv students &lt;- read_csv(&quot;data/students.csv&quot;) Deutsche Excel Versionen nehmen statt Kommata (wie in comma-separated values) jedoch Semikolons (da “,” im Deutschen zur Trennung von Nachkommastellen verwendet wird, im Englischen ist dafür “.” zuständig). In diesem Fall funktioniert jedoch Folgendes: students &lt;- read_csv2(&quot;data/students.csv&quot;) Read_csv funktioniert auch mit Links! read_csv(&quot;https://raw.githubusercontent.com/jannikbuhr/dataIntro19/master/data/students.csv&quot;) Wir können die Daten auch direkt aus einer Excel-Datei einlesen (auch, wenn csv zu bevorzugen ist). # &lt;package&gt;:: means we are not loading the whole package # but rather are just using one function from it readxl::read_excel(&quot;data/students.xlsx&quot;) Schreiben von Daten funktioniert analog dazu. write_csv2(students, &quot;data/students2.csv&quot;) 2.4 The pipe and dplyr verbs The dplyr package and the pipe (%&gt;%) Einfügen mit Ctrl+Shift+M f(g(x)) # entspricht x %&gt;% g %&gt;% f Die wichtigsten dplyr Verben (unsere Werkzeuge zur Transformation von Daten aller Art): select filter arrange mutate summarise Zusätzlich ist noch sehr hilfreich: count, sowie das Adverb group_by, das für sich alleine nichts tut aber das Verhalten der Verben verändert. 2.4.1 select # The following are equivalent students[ , c(&quot;name&quot;, &quot;row&quot;)] select(students, name, row) students %&gt;% select(name, row) students %&gt;% select(-hairlength) 2.4.2 filter # the following 2 are equivalent students[students$row == 1, ] students %&gt;% filter(row == 1) # show only the students in the first row # students with short hair? students %&gt;% filter(row == 2, hairlength == 1) students %&gt;% filter(row == 1 | hairlength == 1) 2.4.3 mutate # convert hairlength from 0 and 1 to &quot;s&quot; and &quot;l&quot; # Add the length of the name as a new column students %&gt;% mutate(hairlength = if_else(hairlength == 0, &quot;s&quot;, &quot;l&quot;), name_length = str_length(name) ) Wir benötigen diese neuen Spalten später, daher geben wir sie nicht nur aus, sondern überschreiben mit dem Ergebnis die Variable students: students &lt;- students %&gt;% mutate(hairlength = if_else(hairlength == 0, &quot;s&quot;, &quot;l&quot;), name_length = str_length(name) ) 2.4.4 arrange Wer hat den längsten Namen? students %&gt;% arrange(desc(name_length)) %&gt;% select(name, name_length) ## # A tibble: 13 x 2 ## name name_length ## &lt;chr&gt; &lt;int&gt; ## 1 Jennifer 8 ## 2 Henrik 6 ## 3 Kieran 6 ## 4 Julius 6 ## 5 Sarwar 6 ## 6 David 5 ## 7 Meret 5 ## 8 Laura 5 ## 9 Julia 5 ## 10 Sarah 5 ## 11 Keno 4 ## 12 Jan 3 ## 13 Jan 3 2.4.5 count students %&gt;% count(hairlength, row) %&gt;% arrange(desc(n)) ## # A tibble: 3 x 3 ## hairlength row n ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 s 1 6 ## 2 l 2 4 ## 3 s 2 3 2.4.6 summarise Was ist die mittlere Namenslänge? students %&gt;% summarise(mean_name_length = mean(name_length)) ## # A tibble: 1 x 1 ## mean_name_length ## &lt;dbl&gt; ## 1 5.15 2.4.7 group_by und summarise Haben Teilnehmende mit langen Haaren auch im Mittel längere Namen? student_summary &lt;- students %&gt;% group_by(row) %&gt;% summarise(mean_name_length = mean(name_length)) %&gt;% ungroup() student_summary ## # A tibble: 2 x 2 ## row mean_name_length ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 5 ## 2 2 5.29 # ungroup is not always necessary but it can be surprising if # you forget that your data had groups 2.5 Was ist Wahrscheinlichkeit? Es gibt zwei Konzepte von Wahrscheinlichkeit (Probability, \\(P\\) ): Probability inside your head: strength of belief; may vary among people Probability „out there“: long-term frequency of an event; can be empirically measured or predicted from a model (Motulsky 2017). 2.5.1 Beispiel: Kategorische / diskrete Daten: Blind aus einem “Hut” ziehen. students %&gt;% count(hairlength) ## # A tibble: 2 x 2 ## hairlength n ## &lt;chr&gt; &lt;int&gt; ## 1 l 4 ## 2 s 9 students %&gt;% count(row) ## # A tibble: 2 x 2 ## row n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 6 ## 2 2 7 # create a &quot;hat&quot; of hairlengths # with the numbers observed in our course hat &lt;- students$hairlength # sample / draw from said hat # the same number of observed long haired in first row sample(hat, 6) # Look at the help for sample (default: replace = FALSE) # How many in this sample have short hair? draw &lt;- sample(hat, 6) sum(draw == &quot;s&quot;) # Explanation of for-loop for simulation for (i in 1:10) { print(i) } ## Simulation # set N N &lt;- 10000 # create empty vector for the sum from each draw # assign the results in a loop results &lt;- vector(&quot;integer&quot;, N) for (i in 1:N) { draw &lt;- sample(hat, 6) results[i] &lt;- sum(draw == &quot;s&quot;) } # Histogram, Mean, Median # mean of results mean(results) ## [1] 4.1567 # histogram of results hist(results, breaks = 0:8) # median median(results) ## [1] 4 # Difference between mean and median, their robustness to outliers! x &lt;- c(1,1.3, 2.1, 1.1, 0, 400) mean(x) ## [1] 67.58333 median(x) ## [1] 1.2 # How surprised should we be? -&gt; Calculate probability for random event # sum of sum greater than or equal to observed frequency sum(results &gt;= 6) / length(results) ## [1] 0.0494 2.5.2 P-Values Eingeführt in den 1920-ern von Ronald Fisher: “The P value is defined as the probability, under the assumption of no effect or no difference (the null hypothesis), of obtaining a result equal to or more extreme than what was actually observed.” \\(-\\) (Original: Statistical Methods for Research Workers) (Fisher und Yates 1990) Nach Konvention: p ≤ 0.05 wird “significant” genannt. In other words, a p-Value is… “… a measure of how surprised you should be if there is no actual difference […], but you got data suggesting there is” \\(-\\) Alex Reinhart (Reinhart 2015) Wir berechne den exakten P-Value: # hypergeometric distribution # note that it calculates cumulative probabilities! # default: P(X &lt;= x) 1 - phyper(5, m = 9, n = 4, k = 6) ## [1] 0.04895105 So sieht die hypergeometrische Verteilung für unser Beispiel aus: barplot(dhyper(x = 0:6, m = 9, n = 4, k = 6), names.arg = 0:6) 2.6 Transfer auf neue Daten Starwars, ein Datenset enthalten im tidyverse. 2.6.1 Übung ?starwars Aufgaben In a new Rmarkdown document: Preview the dataset Select the columns name, heigth, mass, gender Who is the heaviest? Convert height from cm to m Which gender is taller on average in StarWars? Hint: use group_by and summarise You might need the argument “na.rm = TRUE” in mean() Simulate drawing 81 characters (or rather genders) from a hat. Repeat this 1000 times. How often do you obtain 62 or more male characters? How surprised should we be about the data? Calculate an exact p-value for the observed frequency note: use pbinom instead of phyper to sample WHITH replacement Bonus: Create a plot! (any variables that seem interesting) Knit the document into a report 2.6.2 Lösungen starwars # the last step is optional but reduces # clutter in this document output starwars %&gt;% select(name, height, mass, gender) %&gt;% arrange(desc(mass)) %&gt;% head(5) ## # A tibble: 5 x 4 ## name height mass gender ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Jabba Desilijic Tiure 175 1358 masculine ## 2 Grievous 216 159 masculine ## 3 IG-88 200 140 masculine ## 4 Darth Vader 202 136 masculine ## 5 Tarfful 234 136 masculine Abb. 2.2: Jabba the Hut, Quelle: Wikipedia starwars &lt;- starwars %&gt;% mutate(height = height / 100) starwars %&gt;% filter(!is.na(gender)) %&gt;% # ! means &quot;not&quot; group_by(gender) %&gt;% summarise(height = mean(height, na.rm = TRUE)) ## # A tibble: 2 x 2 ## gender height ## &lt;chr&gt; &lt;dbl&gt; ## 1 feminine 1.65 ## 2 masculine 1.77 starwars %&gt;% count(gender) ## # A tibble: 3 x 2 ## gender n ## &lt;chr&gt; &lt;int&gt; ## 1 feminine 17 ## 2 masculine 66 ## 3 &lt;NA&gt; 4 Wie wahrscheinlich ist es, bei gleicher Verteilung von Geschlechtern im Universum, allein durch Zufall diesen oder einen höheren Männerüberschuss im starwars Datenset zu erhalten? Die allein zufällige Verteilung ist unsere Nullhypothese \\(H_0\\). hat &lt;- c(&quot;male&quot;, &quot;female&quot;) total_genders &lt;- starwars %&gt;% filter(!is.na(gender)) %&gt;% nrow() n_males &lt;- starwars %&gt;% filter(gender == &quot;male&quot;) %&gt;% nrow() # This part is an alternative to for loops. # Instead of the loop, we create a function # And then apply (map) that function over all # elements of our vector 1:N draw_from_hat &lt;- function(hat, n) { draw &lt;- sample(hat, n, replace = TRUE) sum(draw == &quot;male&quot;) } N &lt;- 100000 results &lt;- map_int(1:N, ~ draw_from_hat(hat, total_genders) ) hist(results) sum(results &gt;= n_males) ## [1] 100000 Der exakte P-Value wird hier mit pbinom (also der Wahrscheinlichkeit für binomialverteilte Daten) statt phyper (Wahrscheinlichkeit für hypergeometrisch verteilte Daten) ausgerechnet, da wir das Verhältnis der Gender im Universum für konstant annehmen, selbst wenn wir eines daraus für unser Datenset gezogen haben. Wir samplen daher mit replace = TRUE und verwenden die Binomialverteilung. pbinom(q = n_males - 1, size = total_genders, prob = 0.5, lower.tail = FALSE) ## [1] 1 Das Ergebnis ist statistisch signifikant bei einem typischen Signifikanzlevel von \\(p \\leq 0.05\\) (\\(5~\\%\\)). Merke an dieser Stelle, dass wir nicht sagen können, etwas bewiesen zu haben, wir können jedoch sagen, dass wir die Nullhypothese (“es gibt keinen Effekt von Gender auf die Auswahl ins Datenset”) ablehnen. Thus, we reject the null hypothesis. Für mehr Informationen siehe: ?phyper ?pbinom Sowie die unglaublich guten Visualisierungen von Seeing Theory! 2.6.3 Bonus starwars %&gt;% filter(!is.na(homeworld)) %&gt;% group_by(homeworld) %&gt;% summarise(height = mean(height, na.rm = TRUE), mass = mean(mass, na.rm = TRUE)) %&gt;% mutate(homeworld = fct_reorder(homeworld, height)) %&gt;% ggplot(aes(homeworld, height, fill = mass)) + geom_col() + coord_flip() In diesem Dokument zeige ich der Übersicht halber nicht von allen Chunks den kompletten Output, aber der Code lässt sich leicht mittels des Buttons oben rechts an den Blöcken in die Zwischenablage kopieren und selbst in R/RStudio ausprobieren.↩︎ "],
["day3.html", "Tag 3 Tidy Data, Visualisierung und Statistische Größen 3.1 Wiederholung Tag 2 3.2 Visualisierung als Schlüssel zum Verständnis 3.3 Übung", " Tag 3 Tidy Data, Visualisierung und Statistische Größen 3.1 Wiederholung Tag 2 Was ist das tidyverse? Welche Aufgaben erfüllen die wichtigsten dplyr Verben? Transfer: Was tut die folgende Codezeile? starwars %&gt;% group_by(homeworld) %&gt;% mutate(mass = mass / max(mass, na.rm = TRUE)) Was ist ein P-Value? Was ist der Unterschied zwischen binomialverteilten und hypergeometrisch verteilten Daten? Zunächst laden wir das tidyverse, da wir einige Funktionen und auch Beispieldaten daraus verwenden: library(tidyverse) Als Beispiel nehmen wir wieder wieder das Starwars datenset. starwars Wenn du die guten alten Spreadsheets vermisst (also Daten angezeigt wie in Excel und Co.), wirst du die Funktion View mögen. Aber auch sonst kann sie sehr hilfreich sein, falls die Ausgabe der Daten mal nicht ganz in die Konsole passt. View(starwars) select wählt Spalten aus. Dies geht über die Namen, aber auch mithilfe sogenanter select helper, über die du in der Hilfe zu select mehr erfahren kannst (über ?select). nur_namen_und_groesse &lt;- starwars %&gt;% select(name, height) alle_features_mit_farbe &lt;- select(starwars, ends_with(&quot;color&quot;)) das_gleiche_datenset &lt;- select(starwars, everything()) filter hingegen filtert Zeilen, und zwar abhängig von einer beliebigen Zahl an Bedingungen. nur_gerade_massen &lt;- starwars %&gt;% filter(height %% 2 == 0) nur_blaue_augen &lt;- starwars %&gt;% filter(eye_color == &quot;blue&quot;) mutate fügt neue Spalten hinzu oder modifiziert existierende. starwars %&gt;% mutate(all_colors = paste(hair_color, skin_color, eye_color)) %&gt;% select(all_colors, everything()) # Trick um die neu Spalte an den Anfang zu schieben arrange ordnet die Zeilen abhängig von einer oder mehreren ausgewählten Spalten. Hier beispielsweise nach absteigender Größe: starwars %&gt;% arrange(desc(height) summamrise fasst den Dataframe zusammen. starwars %&gt;% summarise(mass = mean(mass, na.rm = TRUE), height = mean(height, na.rm = TRUE)) Wenn der Dataframe vorher gruppiert wurde, operieren summarise und mutate entsprechend je innerhalb der Gruppen. Hier beispielsweise die mittlere Masse pro Heimatplanet: starwars %&gt;% group_by(homeworld) %&gt;% summarise(mass = mean(mass, na.rm = TRUE)) Die oben bereits erwähnte Codezeile starwars %&gt;% group_by(homeworld) %&gt;% mutate(mass = mass / max(mass, na.rm = TRUE) ) dividiert also jede Masse durch die maximale Masse innerhalb des gleichen Heimatplaneten. Zur Definition von P-Values sei auf das vorherige Kapitel unter 2.5.2 verwiesen. Sie werden dir im weiteren Kursverlauf und vorallem auch später in der Forschung noch häufiger über den Weg laufen. Zum Unterschied zwischen einer hypergeometrischen und einer Binomialverteilung: Die hypergeometrische Verteilung ergibt sich beim Ziehen (samplen) ohne Zurücklegen. Als Beispiel sind in der Dokumentation von R unter ?rhyper weiße und schwarze Bälle, die zufällig aus einer Urne gezogen werden, angegeben. x &lt;- rhyper(10000, m = 50, n = 50, k = 80) hist(x) Die Binomialverteilung hingegen entspricht dem Ziehen mit Zurücklegen, oder einfacher gesagt, einer Reihe von Münzwürfen. x &lt;- rbinom(n = 10000, size = 10, prob = 0.5) hist(x, breaks = 0:10) 3.2 Visualisierung als Schlüssel zum Verständnis 3.2.1 Anscombes Quartet Und eine Notiz zu Tidy Data und dem tidyr package. anscombe ## x1 x2 x3 x4 y1 y2 y3 y4 ## 1 10 10 10 8 8.04 9.14 7.46 6.58 ## 2 8 8 8 8 6.95 8.14 6.77 5.76 ## 3 13 13 13 8 7.58 8.74 12.74 7.71 ## 4 9 9 9 8 8.81 8.77 7.11 8.84 ## 5 11 11 11 8 8.33 9.26 7.81 8.47 ## 6 14 14 14 8 9.96 8.10 8.84 7.04 ## 7 6 6 6 8 7.24 6.13 6.08 5.25 ## 8 4 4 4 19 4.26 3.10 5.39 12.50 ## 9 12 12 12 8 10.84 9.13 8.15 5.56 ## 10 7 7 7 8 4.82 7.26 6.42 7.91 ## 11 5 5 5 8 5.68 4.74 5.73 6.89 Anscombes Quartett ist besonders da: Gleicher Mittelwert, gleiche Standardabweichung und der gleiche Korrelationskoeffizient, obwohl die einzelnen Datensets (1 bis 4) sehr unterschiedlich aussehen. mean(anscombe$x1) ## [1] 9 mean(anscombe$x2) ## [1] 9 sd(anscombe$x1) ## [1] 3.316625 sd(anscombe$x2) ## [1] 3.316625 cor(anscombe$x1, anscombe$y1) ## [1] 0.8164205 cor(anscombe$x2, anscombe$y2) ## [1] 0.8162365 An dieser Stelle ein kleiner Trick aus dem Functional Programming: Statt wie oben nacheinander beispielsweise den Mittelwert auszurechnen, kannst du eine Funktion aus der map-Familie verwenden, um eine Funktion (in diesem Fall mean) auf alle Elemente einer Liste anzuwenden. Hierbei gibt map immer eine Liste zurück. Zu jedem Datentyp (z.B. ganze Zahlen integer, Kommazahlen double) gibt es zusätzlich noch eine eigen map-Version map_&lt;datentype&gt;, die nur diesen Typ zurückgibt und es damit sicherer zu Programmieren macht, da Dir Fehler explizit auffallen. map(anscombe, mean) %&gt;% head(2) ## $x1 ## [1] 9 ## ## $x2 ## [1] 9 map_dbl(anscombe, mean) %&gt;% head(2) ## x1 x2 ## 9 9 3.2.2 Exkurs Tidy Data Tidy Data bedeutet: Every row is an observation, every column is a feature Aber das ist nicht immer leicht. “Happy families are all alike; every unhappy family is unhappy in its own way.” \\(-\\) Leo Tolstoy Tidy datasets are all alike, but every messy dataset is messy in its own way.\" \\(-\\) Hadley Wickham Im anscombe Datensatz versteckt sich ein Feature in den Spaltennamen! Das tidyr Package hilft uns dabei dieses Feature explizit zu machen. knitr::include_graphics(&quot;img/tidy-1.png&quot;) Abb. 3.1: Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. (Source: R4DS, Hadley Wickham) anscombe_long &lt;- anscombe %&gt;% pivot_longer(everything(), names_to = c(&quot;.value&quot;, &quot;set&quot;), names_pattern = &quot;(.)(.)&quot; ) Jetzt spielt unser Datenset gut mit dem restlichen Tidyverse, insbesondere ggplot2, zusammen. anscombe_long %&gt;% ggplot(aes(x, y)) + geom_point() + facet_wrap(~set) anscombe_long %&gt;% group_by(set) %&gt;% summarise(m_x = mean(x), m_y = mean(y), sd_x = sd(x)) ## # A tibble: 4 x 4 ## set m_x m_y sd_x ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 7.50 3.32 ## 2 2 9 7.50 3.32 ## 3 3 9 7.5 3.32 ## 4 4 9 7.50 3.32 Abb. 3.2: (Quelle: https://www.stellarscientific.com/accuris-smartreader-96-microplate-absorbance-plate-reader/ # Get the data from github-link atpase_data &lt;- readxl::read_xlsx(&quot;data/04_ATPase_assay.xlsx&quot;, skip = 10, .name_repair = &quot;universal&quot;) %&gt;% select(-Content) %&gt;% rename(time = Time..s.) # Clean data clean_data &lt;- atpase_data %&gt;% pivot_longer(starts_with(&quot;Sample&quot;), names_to = &quot;sample&quot;, values_to = &quot;absorption&quot;) %&gt;% mutate(sample = str_remove(sample, &quot;Sample.&quot;)) # visualize data clean_data %&gt;% ggplot(aes(time, absorption, color = sample)) + geom_line() # normalize data normalized_data &lt;- clean_data %&gt;% group_by(sample) %&gt;% mutate(absorption = absorption / max(absorption)) # visualize again normalized_data %&gt;% ggplot(aes(time, absorption, color = sample)) + geom_line() Aber was ist noch gleich eine Standardabweichung? # explain mean, SD, SEM based on random numbers from normal distribution \\[sd = \\sqrt{\\frac{\\sum_{i=0}^{n}{(x_i-\\bar x)^2}}{(n-1)} }\\] Warum n-1? x = c(1,1,3,1) mean(x) ## [1] 1.5 sd(x) ## [1] 1 \\[var = \\sigma^2\\] Standard Error of the Mean (SEM oder SE) \\[SEM=\\sigma / \\sqrt{n}\\] # generate random normal numbers normal &lt;- rnorm(1000, mean = 0, sd = 1) # hist hist(normal) # mean mean(normal) ## [1] 0.03268081 # sd SD &lt;- sd(normal) # add lines abline(v = SD, col = &quot;red&quot;) abline(v = -SD, col = &quot;red&quot;) # explain percentages within the lines # sem sem &lt;- function(x) sd(x) / length(x) sem(normal) ## [1] 0.0009995849 curve(dnorm, -5, 5) Quantile-Quantile qqnorm(normal) qqline(normal, col = &quot;red&quot;) 3.2.3 The Datasaurus Dozen Das Forschungspaper findet ihr hier (Matejka und Fitzmaurice 2017). # install.packages(&quot;datasauRus&quot;) dinos &lt;- datasauRus::box_plots dinos &lt;- read_csv(&quot;data/datasaurus.csv&quot;) tidy_dinos &lt;- dinos %&gt;% pivot_longer( everything(), names_to = &quot;set&quot;, values_to = &quot;value&quot; ) Boxplots! tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_boxplot() # summarise data by set with SD, SEM, mean summary_dinos &lt;- tidy_dinos %&gt;% group_by(set) %&gt;% summarise( SD = sd(value), SEM = sem(value), value = mean(value) ) # create barplot with errorbars, SD or SEM summary_dinos %&gt;% ggplot(aes(set, value)) + geom_col() + geom_errorbar(aes(ymin = value - SD, ymax = value + SD)) summary_dinos %&gt;% ggplot(aes(set, value)) + geom_col() + geom_errorbar(aes(ymin = value - SEM, ymax = value + SEM)) # finally show the point plot and the jitter plot tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_point(alpha = 0.01) tidy_dinos %&gt;% ggplot(aes(set, value)) + geom_jitter() Friends don’t let friends make barplots! # And make histograms tidy_dinos %&gt;% ggplot(aes(value, fill = set)) + geom_histogram(position = &quot;identity&quot;) + facet_wrap(~set) # And a density plot tidy_dinos %&gt;% ggplot(aes(value, fill = set)) + geom_density() + facet_wrap(~set) 3.3 Übung 3.3.1 Inclusion Bodies Das Datenset lässt sich aus meinem Github Repository herunterladen: read_csv(&quot;https://raw.githubusercontent.com/jannikbuhr/dataIntro19/master/data/03_inclusion_bodies.csv&quot;) %&gt;% write_csv(&quot;data/03_inclusion_bodies.csv&quot;) Natürlich kannst du auch direkt die Daten über den Link laden, aber so hast du sie für schlechte Zeiten (sprich: keine Internetverbindung) gleich gespeichert. Disclaimer: Die Daten stammen aus dem letztjährigen Kurs und sind echte Daten, aber ich lüge euch bei der Bedeutung ein ganz klein wenig an um den biologischen Hintergrund schneller erklären und sie ohne Probleme veröffentlichen zu können. # Erklärung Inclusion Bodies If you want to impress me, do all the exercises in a Rmarkdown document, add your conclusions and thoughts along with the data analysis process and structure it with meaningful headlines using #. Lies die csv-datei unter data/03_inclusion_bodies.csv. Mache tidy data daraus Visualisiere die Daten (mit ggplot2) als Jittered point plot Boxplot Zwei überlagerte Histogramme (Hinweise: Nutze position = “identity” damit R die Histogramme nicht stapelt sondern überlagert) Zwei überlagerte Dichte-Plots (Hinweis: Nutze den Parameter alpha um beide sichtbar zu machen) Einen Barplot mit Fehlerbalken, Hinweise: Erstelle zunächst einen Zusammenfassungs-Dataframe Nutze geom_col, nicht geom_bar BONUS: Mach die Plots hübsch! (e.g. colors, title, caption) 3.3.2 Lösung Eine mögliche Lösung zeige ich euch dann nächste Woche flott live, danach landet der Code hier. Live mache ich das Ganze zur Motivation, um zu zeigen, wie schnell und effektiv die Datenauswertung laufen kann. "],
["day4.html", "Tag 4 Statistische Tests und Power 4.1 Lösung für Tag 3 4.2 Der zentrale Grenzwertsatz (CLT) 4.3 Die T-Verteilung 4.4 Signifikanztests 4.5 Type I und Type II Errors 4.6 Probleme 4.7 Übungen", " Tag 4 Statistische Tests und Power 4.1 Lösung für Tag 3 Ausgangsdokument Ergebnis als html Dokument Ergebnis als pdf Dokument 4.2 Der zentrale Grenzwertsatz (CLT) Von Verteilungen zu Verteilungen von Mittelwerten Was wäre, wenn Johanna als Master Studentin nicht ganz so fleißig gewesen wäre, und statt 300 je Zell Typ nur je 30 mal gezählt hätte? Der Einfachheit nehmen wir an, dass die 300 Zellen komplett repräsentativ für alle WT bzw. KO Zellen sind, also die Gesamtpopulation darstellen. Nun ziehen wir zufällig daraus je 30 Zellen. Das machen wir nun 1000 mal. Wie sieht nun die Verteilung der Mittelwerte aus? 4.2.1 Daten einlesen Zunächst laden wir das Tidyverse und unsere Daten: # load tidyverse library(tidyverse) # import data data &lt;- read_csv(&quot;data/03_inclusion_bodies.csv&quot;) # attach data, handle with CARE! attach(data) attach macht die Spalten des Datensets (in unserem Fall ko and wt) global verfügbar. Daher können wir im Folgenden wt statt data$wt oder data[\"wt\"] schreiben. Verwende attach mit Vorsicht! Denn die Reihenfolge der beiden Vektoren ist nun unabhängig voneinander. Operationen, die Du mit den Vektoren wt und ko ausführst, beeinflussen nicht den jeweils anderen Vektor oder den zugrundeliegenden Data.frame data. 4.2.2 Der einfache Fall Für den einfachen Fall ziehen wir 30 Punkte aus einem der beiden Vektoren und berechnen den Mittelwert. # sample n = 30 points from a vector subset &lt;- sample(wt, 30) # calculate mean mean(subset) ## [1] 2.133333 4.2.3 Abstraktion für viele Fälle Da wir diese Zeilen nicht 1000 mal tippen wollen schreiben wir eine Funktion, die genau das tut. Sie nimmt einen Vektor (x) und zieht n Punkte daraus. get_subset_mean &lt;- function(n, x) { subset &lt;- sample(x, n) mean(subset) } Die Funktion funktioniert: get_subset_mean(2, wt) ## [1] 6 Mit der map Funktion aus dem purrr-Package rufen wir diese Funktion nun 1000 mal auf uns speichern das Ergebnis in einem Vektor, bestehend aus Kommazahlen (daher map_dbl für map double). N &lt;- 1000 Ms &lt;- map_dbl(1:N, ~ get_subset_mean(30, wt) ) 4.2.4 Exkurs: Lambda-Funktionen Das ~-Symbol (Tilde) erstellt eine sogenannte lambda Funktion, also eine Funktion, der wir keinen Namen geben, da wir sie nur einmal brauchen. Im Kontext der Familie von map Funktionen wird ~ übersetzt: add_one &lt;- function(x) x + 1 Ist equivalent zu ~ .x + 1. Hierbei heißt das Argument der Funktion immer .x. Die lambda Funktion ~ get_subset_mean(30, wt) nimmt nacheinander die Zahlen in 1:N als Eingabe, nennt sie .x, aber verwendet das Argument .x überhaubt nicht. In unserem Fall bedeutet das also, dass die Funktion get_subset_mean bloß 1000 mal aufgerufen wird mit den Argumenten 30 und wt. 4.2.5 Populationsverteilung vs. Verteilung der Mittelwerte Mit großem n wird die Verteilung der Mittelwerte symmetrisch, obwohl die Populationsverteilung unsymmetrisch ist. hist(Ms) hist(wt) Nach dem Central Limit Theorem folgen die Mittelwerte einer Normalverteilung. Dies lässt sich zeigen: # show qqnorm of Ms qqnorm(Ms) qqline(Ms, col = &quot;red&quot;) 4.2.6 Exkurs: Quantiles Am Beispiel einer Normalverteilung zeigt dnorm die Wahrscheinlichkeitsdichteverteilung (Im Englischen Probability Density Function, PDF). curve(dnorm, -3, 3) Abb. 4.1: Die Wahrscheinlichkeit, einen Wert zwischen z.B. -1 und 1 zu ziehen, entspricht dem Integral der Funktion in diesem Bereich. Wenn wir nun das Integral von \\(-\\infty\\) bis \\(x\\) berechnen erhalten wir die kumulative Wahrscheinlichkeitsfunktion pnorm. curve(pnorm, -3, 3) Abb. 4.2: pnorm von x ist also die Wahrscheinlichkeit, einen Wert kleiner oder gleich x aus dnorm zu ziehen. qnomr, die Quantil-Funktion, ist nun die Inverse dieser Funkion, hat also die Achsen vertauscht. curve(qnorm, 0, 1) 4.3 Die T-Verteilung Der zentrale Grenzwertsatz gilt erst bei großen sample sizes. Darunter ist die Verteilung der Mittelwerte an den Rändern dicker, als eine Normalverteilung. Daher verwenden wir (sicherheitshalber) für die meisten Daten und statistischen Tests die T-Verteilung, nicht die Normalverteilung. Für 3 Freiheitsgrade: curve(dnorm, -3, 3) tdist &lt;- function(x) dt(x, df = 3) curve(tdist, -3, 3, add = TRUE, col = &quot;red&quot;) Abb. 4.3: T-Verteilung in Rot Für 30 Freiheitsgrade curve(dnorm, -3, 3) tdist &lt;- function(x) dt(x, df = 30) curve(tdist, -3, 3, add = TRUE, col = &quot;red&quot;) 4.3.1 Konfidenz Intervalle (CI) Gleichermaßen können wir mit der t-Verteilung 95% Konfidenzintervalle berechnen. Das 95% Konfidenzintervall eines Sample-Mittelwerts umschließt den wahren Mittelwert (den Mittelwert der Gesamtpopulation) in 95% der Fälle (wenn Du das Experiment unendlich oft wiederholen würdest). # show hist of Ms hist(Ms) # add line for true mean of wt abline(v = mean(wt), col = &quot;red&quot;, lwd = 3) Abb. 4.4: Wahrer Mittelwert in Rot # use t.test for conf.int subset &lt;- sample(wt, 30) mean(subset) ## [1] 3.166667 lims &lt;- t.test(subset)$conf.int hist(subset, breaks = 20) abline(v = lims[1], col = &quot;red&quot;) abline(v = lims[2], col = &quot;red&quot;) 4.4 Signifikanztests Signifikanztests beantworten die Frage: Unter der Annahme, dass es zwischen zwei (oder mehr) Gruppen keinen Unterschied gibt (also sie aus der gleichen Verteilung stammen), wie wahrscheinlich ist es dann, einen so großen, oder größeren, Unterschied, wie beobachtet, zu finden? 4.4.1 Students T-Test Für normalverteilte Daten verwenden wir den Students T-Test. t.test(wt, ko) ## ## Welch Two Sample t-test ## ## data: wt and ko ## t = -26.929, df = 476.21, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -13.19393 -11.39940 ## sample estimates: ## mean of x mean of y ## 3.043333 15.340000 subset_wt &lt;- sample(wt, 3) subset_ko &lt;- sample(ko, 3) t.test(subset_wt, subset_ko) ## ## Welch Two Sample t-test ## ## data: subset_wt and subset_ko ## t = -6.8224, df = 2.6162, p-value = 0.009833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -16.081856 -5.251477 ## sample estimates: ## mean of x mean of y ## 1.00000 11.66667 4.4.2 Wilcoxon Rank Sum Test Sind unsere Daten nicht normalverteilt, wie es hier der Fall ist, ist ein sogenannter nicht-parametrischer Test angebracht. Diese Tests machen nicht die Annahme der Normalität. Der Wilcoxon Rank Sum Test (Auch Mann-Whitney U Test genannt) wandelt die eigentlichen Werte der Datenpunkte zunächst in Ränge um. Also: “Der wievieltniedrigste Punkt ist es?” Beispiel: x &lt;- c(1, 2, 20, 1239132, 3, 5) rank(x) ## [1] 1 2 5 6 3 4 Und an unseren Daten: wilcox.test(wt, ko) ## ## Wilcoxon rank sum test with continuity ## correction ## ## data: wt and ko ## W = 4707.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 4.5 Type I und Type II Errors Type I: False Positives (rejection of a true null hypothesis) Type II: False Negatives (non-rejection of false null hypothesis) 4.5.1 Type I: False Positives Ein Typ I Fehler würde bedeuten, dass wir sagen, ein Unterschied zwischen den Gruppen existiere, obwohl alle Werte aus der gleichen Verteilung stammen und daher kein Unterschied besteht. Bei einem P-Value von \\(\\leq\\) 0.05 (= 5 %) wird typischerweise gesagt, dass ein “statistisch signifikanter” Unterschied besteht. Mit diesem typischen Cutoff von 0.05 akzeptieren wir also, allein durch unsere Definition, bereits (mindestens) 5 % falsch Positive. Dieser Cutoff, das Signifikanzlevel, wird auch \\(\\alpha\\) genannt. \\[\\alpha=\\text{Type I error rate}\\] curve(dnorm, -3 , 3) # define n n &lt;- 3 # draw twice from the same normal distributions draw1 &lt;- rnorm(n) draw2 &lt;- rnorm(n) # do the t.test t.test(draw1, draw2) ## ## Welch Two Sample t-test ## ## data: draw1 and draw2 ## t = -0.31189, df = 3.0148, p-value = 0.7754 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.651341 2.997886 ## sample estimates: ## mean of x mean of y ## -0.1263381 0.2003894 # do the rank sum test wilcox.test(draw1, draw2) ## ## Wilcoxon rank sum exact test ## ## data: draw1 and draw2 ## W = 4, p-value = 1 ## alternative hypothesis: true location shift is not equal to 0 Teste diesen Code mit unterschiedlichen Werten für n. 4.5.2 Type II errors Wenn zwischen zwei Gruppen ein tatsächlicher Unterschied besteht, wir aber bei unserem Test ein nicht signifikantes Ergebnis erhalten, begehen wir einen Fehler vom Typ II. Ein Typ II Fehler ist also ein falsch negatives Ergebnis. \\[\\beta=\\text{Type II error rate}\\] # define n n &lt;- 15 # draw from different normal distributions draw1 &lt;- rnorm(n, 0, 1) draw2 &lt;- rnorm(n, 1, 1) # convert to tibble, pivot_longer, plot points tibble(draw1, draw2) %&gt;% pivot_longer(c(1,2)) %&gt;% ggplot(aes(name, value, group = 1)) + geom_jitter(width = 0.2) + stat_summary(geom = &quot;line&quot;, fun.y = mean, lwd = 1, lty = 2) + stat_summary(geom = &quot;point&quot;, fun.y = mean, color = &quot;red&quot;, size = 2) # do the t.test t.test(draw1, draw2) ## ## Welch Two Sample t-test ## ## data: draw1 and draw2 ## t = -2.5888, df = 27.319, p-value = 0.01525 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.9133144 -0.2219188 ## sample estimates: ## mean of x mean of y ## 0.06255136 1.13016800 # do the rank sum test wilcox.test(draw1, draw2) ## ## Wilcoxon rank sum exact test ## ## data: draw1 and draw2 ## W = 57, p-value = 0.0209 ## alternative hypothesis: true location shift is not equal to 0 4.5.3 Statistical Power Als statistische Power bezeichnen wir die Wahrscheinlichkeit eines Tests, einen wahren Unterschied zwischen Gruppen bei dem gewählten \\(\\alpha\\) auch tatsächlich als statistisch signifikant zu kennzeichnen und damit ein wahr positives Ergebnis zu produzieren. \\[Power = 1-\\beta\\] Für den T-Test können wir in R die folgende Funktion verwenden, die uns hier beispielsweise die Frage beantwortet: “Wie viele Proben muss ich pro Gruppe (mindestens) nehmen, um einen erwarteten Unterschied der Mittelwerte von 1 mit einer Standardabweichung von 1 in 80% der Fälle auch tatsächlich als solchen zu erkennen?” # show power.t.test power.t.test(delta = 1, sd = 1, power = 0.8) ## ## Two-sample t test power calculation ## ## n = 16.71477 ## delta = 1 ## sd = 1 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group 4.6 Probleme 4.6.1 The Jelly Bean Problem (Multiple Testing) Die False Discovery Rate (FDA) kontrollieren: Bonferroni Korrektur # show p.adjust p.adjust(c(0.05, 0.0001, 0.003323, 0.7), method = &quot;bonferroni&quot;) ## [1] 0.200000 0.000400 0.013292 1.000000 Jeder P-Value wird mutipliziert mit der Zahl der Tests Benjamini-Hochberg Prozedur Ordne alle p-values in aufsteigender Reihenfolge Wähle eine FDR (\\(q\\)) und nenne die Anzahl deiner Tests \\(m\\) Finde den größten p-value für den gilt: \\(p \\leq iq/m\\) mit dem Index des p-values \\(i\\). # show p.adjust for BH p.adjust(c(0.05, 0.0001, 0.003323, 0.7), method = &quot;BH&quot;) ## [1] 0.06666667 0.00040000 0.00664600 0.70000000 4.6.2 The Base Rate Fallacy Beispiel: Mammogramm Sensitivity = Power = true positive rate Specificity = true negative rate = \\(1-\\alpha\\) # calculate total &lt;- 1000 positives &lt;- 10 negatives &lt;- total - positives sensitivity &lt;- 0.9 specificity &lt;- 1 - 0.08 true_positives &lt;- sensitivity * positives false_positives &lt;- (1 - specificity) * negatives p_positive &lt;- true_positives / (true_positives + false_positives) p_positive ## [1] 0.1020408 4.6.3 Resourcen Statistics Done Wrong: https://www.statisticsdonewrong.com/ Intuitive Biostatistics, auch in der Uni-Bib: https://katalog.ub.uni-heidelberg.de/cgi-bin/titel.cgi?katkey=68260114&amp;sess=050a1316767b181982c9bce94283e9ae&amp;query=Intuitive%20Biostatistics https://www.graphpad.com/guides/prism/8/statistics/index.htm 4.7 Übungen 4.7.1 Zeige die folgenden Statements anhand von Simulationen in R 4.7.1.1 Wenn Du die Sample Size n erhöhst, verringert sich der Standard Error of the Mean mit der Wurzel von n. Ziehe 10 Zellen aus dem wt (oder ko) Vektor Berechne den Mittelwert Wiederhole das Ganze 1000 mal Berechne die SD der 1000 Mittelwerte Ziehe nun 40 statt 10 Zellen und wiederhole die Schritte Schreibe eine Funktion, die n Zellen zieht, die Schritte ausführt und die SD ausgibt Füttere die Zahlen von 1 bis 100 in die Funktion (mit map_dbl) und plotte das Ergebnis 4.7.1.2 Lösung library(tidyverse) read_csv(&quot;data/03_inclusion_bodies.csv&quot;) %&gt;% attach() draw &lt;- sample(wt, 10) mean(draw) ## [1] 3.5 get_sample_mean &lt;- function(x, n) { draw &lt;- sample(x, n, replace = TRUE) mean(draw) } get_sample_mean(wt, 10) ## [1] 3.9 N &lt;- 1000 many_means &lt;- map_dbl(1:N, ~ get_sample_mean(wt, 10) ) sd(many_means) ## [1] 1.245121 many_means &lt;- map_dbl(1:N, ~get_sample_mean(wt, 40)) sd(many_means) ## [1] 0.6121342 # explain default arguments and scoping get_sd_of_many_means &lt;- function(n, x, N = 1000) { many_means &lt;- map_dbl(1:N, ~get_sample_mean(x, n)) sd(many_means) } n_max &lt;- 100 sds_by_n &lt;- map_dbl(1:n_max, get_sd_of_many_means, x = wt) plot(x = 1:n_max, y = sds_by_n, type = &quot;p&quot;) curve(sd(wt)/sqrt(x), add = TRUE, col = &quot;red&quot;) # talk about difference between sample sd and population sd 4.7.1.3 Ein 95% Konfidenzintervall eines Samples enthält den Populationsmittelwert mit einer Wahrscheinlichkeit von 95%. Ziehe 30 Zellen aus dem wt (oder ko) Vektor Berechne die Limits des CI (Confidence Interval) Schreibe eine Funktion, oben genanntes tut und führe sie 1000 mal aus. Schreibe eine Funktion, die testet, ob ein Set an Limits den wahren Mittelwert einschließt Wende sie auf die 1000 Sets der Limits an Wie oft (prozentual) erhältst du TRUE? 4.7.1.4 Lösung draw &lt;- sample(wt, 30) test_results &lt;- t.test(draw) test_results ## ## One Sample t-test ## ## data: draw ## t = 3.9747, df = 29, p-value = 0.0004285 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 1.165051 3.634949 ## sample estimates: ## mean of x ## 2.4 summary(test_results) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 1 -none- numeric ## null.value 1 -none- numeric ## stderr 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(test_results) ## List of 10 ## $ statistic : Named num 3.97 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 29 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 0.000428 ## $ conf.int : num [1:2] 1.17 3.63 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num 2.4 ## ..- attr(*, &quot;names&quot;)= chr &quot;mean of x&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;mean&quot; ## $ stderr : num 0.604 ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;One Sample t-test&quot; ## $ data.name : chr &quot;draw&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; test_results$conf.int ## [1] 1.165051 3.634949 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 get_sample_ci &lt;- function(x, n) { draw &lt;- sample(x, n, replace = TRUE) t.test(draw)$conf.int } get_sample_ci(wt, 30) ## [1] 1.272275 3.727725 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 CIs &lt;- map(1:1000, ~get_sample_ci(wt, 30)) head(CIs) ## [[1]] ## [1] 1.319300 4.147366 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## ## [[2]] ## [1] 1.55864 5.24136 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## ## [[3]] ## [1] 1.564104 4.502562 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## ## [[4]] ## [1] 1.402216 3.997784 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## ## [[5]] ## [1] 1.211124 2.588876 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 ## ## [[6]] ## [1] 2.414082 6.319252 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 # explain list subsetting CIs[1] ## [[1]] ## [1] 1.319300 4.147366 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 CIs[[1]] ## [1] 1.319300 4.147366 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 CIs[[1]][1] ## [1] 1.3193 test_ci &lt;- function(limits, true_mean) { limits[1] &lt; true_mean &amp; limits[2] &gt; true_mean } # sidenote # note difference between &amp; and &amp;&amp; c(TRUE, TRUE) &amp; c(FALSE, TRUE) ## [1] FALSE TRUE c(TRUE, TRUE) &amp;&amp; c(FALSE, TRUE) ## [1] FALSE results &lt;- map_lgl(CIs, test_ci, true_mean = mean(wt)) head(results) ## [1] TRUE TRUE TRUE TRUE FALSE TRUE mean(results) ## [1] 0.936 # talk about organisation of code # source(&quot;test.R&quot;) 4.7.1.5 Weitere Hinweise # check out ?map # for the subtle differences between: map(1:100, ~ .x + 1) ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 3 ## ## [[3]] ## [1] 4 ## ## [[4]] ## [1] 5 ## ## [[5]] ## [1] 6 ## ## [[6]] ## [1] 7 ## ## [[7]] ## [1] 8 ## ## [[8]] ## [1] 9 ## ## [[9]] ## [1] 10 ## ## [[10]] ## [1] 11 ## ## [[11]] ## [1] 12 ## ## [[12]] ## [1] 13 ## ## [[13]] ## [1] 14 ## ## [[14]] ## [1] 15 ## ## [[15]] ## [1] 16 ## ## [[16]] ## [1] 17 ## ## [[17]] ## [1] 18 ## ## [[18]] ## [1] 19 ## ## [[19]] ## [1] 20 ## ## [[20]] ## [1] 21 ## ## [[21]] ## [1] 22 ## ## [[22]] ## [1] 23 ## ## [[23]] ## [1] 24 ## ## [[24]] ## [1] 25 ## ## [[25]] ## [1] 26 ## ## [[26]] ## [1] 27 ## ## [[27]] ## [1] 28 ## ## [[28]] ## [1] 29 ## ## [[29]] ## [1] 30 ## ## [[30]] ## [1] 31 ## ## [[31]] ## [1] 32 ## ## [[32]] ## [1] 33 ## ## [[33]] ## [1] 34 ## ## [[34]] ## [1] 35 ## ## [[35]] ## [1] 36 ## ## [[36]] ## [1] 37 ## ## [[37]] ## [1] 38 ## ## [[38]] ## [1] 39 ## ## [[39]] ## [1] 40 ## ## [[40]] ## [1] 41 ## ## [[41]] ## [1] 42 ## ## [[42]] ## [1] 43 ## ## [[43]] ## [1] 44 ## ## [[44]] ## [1] 45 ## ## [[45]] ## [1] 46 ## ## [[46]] ## [1] 47 ## ## [[47]] ## [1] 48 ## ## [[48]] ## [1] 49 ## ## [[49]] ## [1] 50 ## ## [[50]] ## [1] 51 ## ## [[51]] ## [1] 52 ## ## [[52]] ## [1] 53 ## ## [[53]] ## [1] 54 ## ## [[54]] ## [1] 55 ## ## [[55]] ## [1] 56 ## ## [[56]] ## [1] 57 ## ## [[57]] ## [1] 58 ## ## [[58]] ## [1] 59 ## ## [[59]] ## [1] 60 ## ## [[60]] ## [1] 61 ## ## [[61]] ## [1] 62 ## ## [[62]] ## [1] 63 ## ## [[63]] ## [1] 64 ## ## [[64]] ## [1] 65 ## ## [[65]] ## [1] 66 ## ## [[66]] ## [1] 67 ## ## [[67]] ## [1] 68 ## ## [[68]] ## [1] 69 ## ## [[69]] ## [1] 70 ## ## [[70]] ## [1] 71 ## ## [[71]] ## [1] 72 ## ## [[72]] ## [1] 73 ## ## [[73]] ## [1] 74 ## ## [[74]] ## [1] 75 ## ## [[75]] ## [1] 76 ## ## [[76]] ## [1] 77 ## ## [[77]] ## [1] 78 ## ## [[78]] ## [1] 79 ## ## [[79]] ## [1] 80 ## ## [[80]] ## [1] 81 ## ## [[81]] ## [1] 82 ## ## [[82]] ## [1] 83 ## ## [[83]] ## [1] 84 ## ## [[84]] ## [1] 85 ## ## [[85]] ## [1] 86 ## ## [[86]] ## [1] 87 ## ## [[87]] ## [1] 88 ## ## [[88]] ## [1] 89 ## ## [[89]] ## [1] 90 ## ## [[90]] ## [1] 91 ## ## [[91]] ## [1] 92 ## ## [[92]] ## [1] 93 ## ## [[93]] ## [1] 94 ## ## [[94]] ## [1] 95 ## ## [[95]] ## [1] 96 ## ## [[96]] ## [1] 97 ## ## [[97]] ## [1] 98 ## ## [[98]] ## [1] 99 ## ## [[99]] ## [1] 100 ## ## [[100]] ## [1] 101 # and add_one &lt;- function(x) x + 1 map(1:100, add_one) ## [[1]] ## [1] 2 ## ## [[2]] ## [1] 3 ## ## [[3]] ## [1] 4 ## ## [[4]] ## [1] 5 ## ## [[5]] ## [1] 6 ## ## [[6]] ## [1] 7 ## ## [[7]] ## [1] 8 ## ## [[8]] ## [1] 9 ## ## [[9]] ## [1] 10 ## ## [[10]] ## [1] 11 ## ## [[11]] ## [1] 12 ## ## [[12]] ## [1] 13 ## ## [[13]] ## [1] 14 ## ## [[14]] ## [1] 15 ## ## [[15]] ## [1] 16 ## ## [[16]] ## [1] 17 ## ## [[17]] ## [1] 18 ## ## [[18]] ## [1] 19 ## ## [[19]] ## [1] 20 ## ## [[20]] ## [1] 21 ## ## [[21]] ## [1] 22 ## ## [[22]] ## [1] 23 ## ## [[23]] ## [1] 24 ## ## [[24]] ## [1] 25 ## ## [[25]] ## [1] 26 ## ## [[26]] ## [1] 27 ## ## [[27]] ## [1] 28 ## ## [[28]] ## [1] 29 ## ## [[29]] ## [1] 30 ## ## [[30]] ## [1] 31 ## ## [[31]] ## [1] 32 ## ## [[32]] ## [1] 33 ## ## [[33]] ## [1] 34 ## ## [[34]] ## [1] 35 ## ## [[35]] ## [1] 36 ## ## [[36]] ## [1] 37 ## ## [[37]] ## [1] 38 ## ## [[38]] ## [1] 39 ## ## [[39]] ## [1] 40 ## ## [[40]] ## [1] 41 ## ## [[41]] ## [1] 42 ## ## [[42]] ## [1] 43 ## ## [[43]] ## [1] 44 ## ## [[44]] ## [1] 45 ## ## [[45]] ## [1] 46 ## ## [[46]] ## [1] 47 ## ## [[47]] ## [1] 48 ## ## [[48]] ## [1] 49 ## ## [[49]] ## [1] 50 ## ## [[50]] ## [1] 51 ## ## [[51]] ## [1] 52 ## ## [[52]] ## [1] 53 ## ## [[53]] ## [1] 54 ## ## [[54]] ## [1] 55 ## ## [[55]] ## [1] 56 ## ## [[56]] ## [1] 57 ## ## [[57]] ## [1] 58 ## ## [[58]] ## [1] 59 ## ## [[59]] ## [1] 60 ## ## [[60]] ## [1] 61 ## ## [[61]] ## [1] 62 ## ## [[62]] ## [1] 63 ## ## [[63]] ## [1] 64 ## ## [[64]] ## [1] 65 ## ## [[65]] ## [1] 66 ## ## [[66]] ## [1] 67 ## ## [[67]] ## [1] 68 ## ## [[68]] ## [1] 69 ## ## [[69]] ## [1] 70 ## ## [[70]] ## [1] 71 ## ## [[71]] ## [1] 72 ## ## [[72]] ## [1] 73 ## ## [[73]] ## [1] 74 ## ## [[74]] ## [1] 75 ## ## [[75]] ## [1] 76 ## ## [[76]] ## [1] 77 ## ## [[77]] ## [1] 78 ## ## [[78]] ## [1] 79 ## ## [[79]] ## [1] 80 ## ## [[80]] ## [1] 81 ## ## [[81]] ## [1] 82 ## ## [[82]] ## [1] 83 ## ## [[83]] ## [1] 84 ## ## [[84]] ## [1] 85 ## ## [[85]] ## [1] 86 ## ## [[86]] ## [1] 87 ## ## [[87]] ## [1] 88 ## ## [[88]] ## [1] 89 ## ## [[89]] ## [1] 90 ## ## [[90]] ## [1] 91 ## ## [[91]] ## [1] 92 ## ## [[92]] ## [1] 93 ## ## [[93]] ## [1] 94 ## ## [[94]] ## [1] 95 ## ## [[95]] ## [1] 96 ## ## [[96]] ## [1] 97 ## ## [[97]] ## [1] 98 ## ## [[98]] ## [1] 99 ## ## [[99]] ## [1] 100 ## ## [[100]] ## [1] 101 # and map_dbl(1:100, add_one) ## [1] 2 3 4 5 6 7 8 9 10 11 12 13 ## [13] 14 15 16 17 18 19 20 21 22 23 24 25 ## [25] 26 27 28 29 30 31 32 33 34 35 36 37 ## [37] 38 39 40 41 42 43 44 45 46 47 48 49 ## [49] 50 51 52 53 54 55 56 57 58 59 60 61 ## [61] 62 63 64 65 66 67 68 69 70 71 72 73 ## [73] 74 75 76 77 78 79 80 81 82 83 84 85 ## [85] 86 87 88 89 90 91 92 93 94 95 96 97 ## [97] 98 99 100 101 # especially the meaning of ~ (speak: lambda) map(1:5, ~ print(&quot;hi&quot;)) ## [1] &quot;hi&quot; ## [1] &quot;hi&quot; ## [1] &quot;hi&quot; ## [1] &quot;hi&quot; ## [1] &quot;hi&quot; ## [[1]] ## [1] &quot;hi&quot; ## ## [[2]] ## [1] &quot;hi&quot; ## ## [[3]] ## [1] &quot;hi&quot; ## ## [[4]] ## [1] &quot;hi&quot; ## ## [[5]] ## [1] &quot;hi&quot; # more examples map(1:10, paste, &quot;hi&quot;) ## [[1]] ## [1] &quot;1 hi&quot; ## ## [[2]] ## [1] &quot;2 hi&quot; ## ## [[3]] ## [1] &quot;3 hi&quot; ## ## [[4]] ## [1] &quot;4 hi&quot; ## ## [[5]] ## [1] &quot;5 hi&quot; ## ## [[6]] ## [1] &quot;6 hi&quot; ## ## [[7]] ## [1] &quot;7 hi&quot; ## ## [[8]] ## [1] &quot;8 hi&quot; ## ## [[9]] ## [1] &quot;9 hi&quot; ## ## [[10]] ## [1] &quot;10 hi&quot; is_even &lt;- function(x) x %% 2 == 0 map_lgl(1:10, is_even) ## [1] FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE ## [9] FALSE TRUE # Note: Often, you don&#39;t need a map function # Because many functions in R are vectorised by default: x &lt;- 1:10 y &lt;- 1:10 # thus, just write x + y ## [1] 2 4 6 8 10 12 14 16 18 20 # instead of map2_dbl(x,y, `+`) ## [1] 2 4 6 8 10 12 14 16 18 20 4.7.2 Veranschauliche die folgenden Konzepte anhand von Simulationen auf den vorliegenden Daten 4.7.2.1 Sensitivity: Wie groß ist die Wahrscheinlichkeit abhängig von der Sample Size n mit einem Wilcoxon Rank Sum Test tatsächlich einen p-value &lt;= 0.05 zu erhalten, wenn ein Unterschied existiert? Ziehe 1000 mal ein Sample von 30 je aus wt und ko und teste auf Signifikanz Wie viele der 1000 Versuche sind statistisch signifikant? Wie verändert sich diese Zahl, wenn 10 statt 30 gezogen werden? 4.7.2.2 Lösung test_samle_wilcox &lt;- function(n) { draw_wt &lt;- sample(wt, n, replace = TRUE) draw_ko &lt;- sample(ko, n, replace = TRUE) wilcox.test(draw_wt, draw_ko, exact = FALSE)$p.value } many_p_values &lt;- map_dbl(1:1000, ~test_samle_wilcox(30)) hist(many_p_values) many_p_values &lt;- map_dbl(1:1000, ~test_samle_wilcox(10)) hist(many_p_values) many_p_values &lt;- map_dbl(1:1000, ~test_samle_wilcox(4)) hist(many_p_values) alpha = 0.05 mean(many_p_values &lt;= alpha) ## [1] 0.642 # talk about power, effect size, difference between wilcox and t-test 4.7.2.3 Specificity: Unter der Voraussetzung, dass kein Unterschied zwischen den Bedingungen vorliegt, wie groß ist die Wahrscheinlichkeit, dennoch ein statistisch signifikantes Ergebnis zu erhalten? Stell dir vor, alle Zellen seien wie wt-Zellen Ziehe zwei mal 30 aus den wt-Zellen und lasse einen Wilcoxon Rank Sum Test laufen Widerhole das das Prozedere 1000 mal Wie oft ist das Ergebnis statistisch signifikant? 4.7.2.4 Lösung test_same_wilcox &lt;- function(n) { draw_wt &lt;- sample(wt, n, replace = TRUE) draw_ko &lt;- sample(wt, n, replace = TRUE) wilcox.test(draw_wt, draw_ko, exact = FALSE)$p.value } many_p_values &lt;- map_dbl(1:1000, ~test_same_wilcox(30)) hist(many_p_values) alpha = 0.05 mean(many_p_values &lt;= alpha) ## [1] 0.065 # this is what we would expect from p-values! 4.7.2.5 Resource zu p-value Histogrammen http://varianceexplained.org/statistics/interpreting-pvalue-histogram/ "],
["day5.html", "Tag 5 Korrelation und Regression 5.1 Lösung Tag 4 5.2 Korrelation und Regression 5.3 Tricks für viele Daten / Dateien 5.4 Non-Linear Regression 5.5 Übung", " Tag 5 Korrelation und Regression 5.1 Lösung Tag 4 siehe Tag 4. 5.2 Korrelation und Regression https://figshare.com/articles/Storks_and_human_babies_data/839299/1 oder https://github.com/jannikbuhr/dataIntro19/tree/master/data library(tidyverse) library(broom) storks &lt;- read_csv(&quot;data/05_storks.csv&quot;) storks ggplot(storks, aes(Storks, Birth)) + geom_point() # explain log transform and relevance in biology ggplot(storks, aes(Storks, Birth)) + geom_point() + scale_x_continuous(trans = &quot;log10&quot;) + scale_y_continuous(trans = &quot;log10&quot;) + annotation_logticks() + theme_classic() model &lt;- lm(Birth ~ Storks, data = storks) model summary(model) ggplot(storks, aes(Storks, Birth)) + geom_point() + scale_x_continuous(trans = &quot;log10&quot;, breaks = scales::log_breaks()) + scale_y_continuous(trans = &quot;log10&quot;, breaks = scales::log_breaks()) + annotation_logticks() + theme_classic() + geom_smooth(method = &quot;lm&quot;) model &lt;- lm(log(Birth) ~ log(Storks), data = storks) model summary(model) # explain correlation # explain range of correlation coefficient cor(storks$Storks, storks$Birth) cor(log(storks$Storks), log(storks$Birth)) cor.test(storks$Storks, storks$Birth) cor(storks$Storks, storks$Birth, method = &quot;spearman&quot;) model &lt;- lm(storks$Birth ~ storks$Storks) summary(model) cor(storks$Storks, storks$Birth)^2 with(storks, { plot(Storks, Birth) abline(lm(Birth~Storks)) }) with(storks, { plot(log(Storks), log(Birth)) abline(lm(log(Birth)~log(Storks))) }) # show xkcd # talk about confounding factors / variables storks 5.3 Tricks für viele Daten / Dateien paths &lt;- dir(&quot;data&quot;, full.names = TRUE, pattern = &quot;.csv&quot;) map(paths, read_csv) # map_dfr 5.4 Non-Linear Regression Puromycin &lt;- as_tibble(Puromycin) Puromycin Puromycin %&gt;% count(state) puro_treat &lt;- Puromycin %&gt;% filter(state == &quot;treated&quot;) plot(puro_treat$conc, puro_treat$rate) ggplot(puro_treat, aes(conc, rate)) + geom_point() michaelis_menten_fun &lt;- function(conc, Vm, K) { (Vm * conc) / (K + conc) } x = seq(0, 1, by = 0.001) y = michaelis_menten_fun(x, Vm = 200, K = 0.2) plot(x, y) plot(puro_treat$conc, puro_treat$rate) curve(michaelis_menten_fun(conc = x, Vm = 200, K = 0.1), add = TRUE, col = &quot;red&quot;, from = 0, to = 1, n = 100) model &lt;- nls(rate ~ michaelis_menten_fun(conc, Vm, K), start = list(Vm = 200, K = 0.1), data = puro_treat) model coef(model) summary(model)$coefficients Vm_est &lt;- coef(model)[1] K_est &lt;- coef(model)[2] plot(puro_treat$conc, puro_treat$rate) curve(michaelis_menten_fun(x, Vm = Vm_est, K = K_est), add = TRUE, col = &quot;red&quot;) ggplot(puro_treat, aes(conc, rate)) + geom_point() + stat_function(fun = ~ michaelis_menten_fun(conc = .x, Vm = Vm_est, K = K_est), col = &quot;red&quot;) + theme_classic() model2 &lt;- nls(rate ~ SSmicmen(conc, Vm, K), data = puro_treat) model2 predict(model2, newdata = list(conc = 1:10)) nested_data &lt;- Puromycin %&gt;% group_nest(state) nested_data nested_data$data[[1]] all_models &lt;- nested_data %&gt;% mutate(model = map(data, ~ nls(rate ~ michaelis_menten_fun(conc, Vm, K), start = list(Vm = 200, K = 0.1), data = .x)) ) all_models fit_my_model &lt;- function(df) { nls(rate ~ michaelis_menten_fun(conc, Vm, K), start = list(Vm = 200, K = 0.1), data = df) } fit_my_model(puro_treat) all_models &lt;- nested_data %&gt;% mutate(model = map(data, fit_my_model) ) all_models all_models$model[[1]] all_models &lt;- all_models %&gt;% mutate(fitted = map(model, augment), coef = map(model, coef)) all_models all_models %&gt;% unnest_wider(coef) all_models %&gt;% select(state, fitted) %&gt;% unnest(fitted) all_models %&gt;% select(state, fitted) %&gt;% unnest(fitted) %&gt;% ggplot(aes(conc, rate, color = state)) + geom_point() + geom_line(aes(y = .fitted)) make_data &lt;- function(model, from, to, n) { conc = seq(from, to, length.out = n) .fitted = predict(model, newdata = list(conc = conc)) tibble(conc, .fitted) } new_data &lt;- all_models %&gt;% mutate(new_data = map(model, make_data, 0, 1, 100)) %&gt;% select(state, new_data) %&gt;% unnest(new_data) all_models %&gt;% select(state, fitted) %&gt;% unnest(fitted) %&gt;% ggplot(aes(conc, rate, color = state)) + geom_point() + geom_line(data = new_data, aes(y = .fitted)) 5.5 Übung 5.5.1 Mit den Datasaurus Dozen Datensets datasauRus::datasaurus_dozen Visualisiere alle Sets gemeinsam in einem ggplot Scatterplot, nutze dazu facet_wrap. Füge mittels geom_smooth lineare Trendlinien hinzu Fitte eine Lineare Regression and jedes der Datensets. Nutze dazu die Techniken aus R4DataScience: Many Models und das broom package. Analysiere die Fits. "],
["day6.html", "Tag 6 Folgt 6.1 Lösung Tag 5 6.2 Datenanalyse, Beispiel: Global Plastic Waste 6.3 Feedbackrunde", " Tag 6 Folgt 6.1 Lösung Tag 5 library(tidyverse) library(broom) 6.1.1 Mit den Datasaurus Dozen Datensets datasauRus::datasaurus_dozen Visualisiere alle Sets gemeinsam in einem ggplot Scatterplot, nutze dazu facet_wrap. Füge mittels geom_smooth lineare Trendlinien hinzu Fitte eine Lineare Regression and jedes der Datensets. Nutze dazu die Techniken aus R4DataScience: Many Models und das broom package. Analysiere die Fits. dinos &lt;- datasauRus::datasaurus_dozen dinos dinos %&gt;% count(dataset) dinos %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_wrap(&quot;dataset&quot;, scales = &quot;free_y&quot;) + ggthemes::theme_few() nested_dinos &lt;- dinos %&gt;% group_nest(dataset) nested_dinos beispiel &lt;- nested_dinos$data[[1]] model &lt;- lm(y ~ x, data = beispiel) summary(model) glance(model) tidy(model) coef(model) all_models &lt;- nested_dinos %&gt;% mutate( model = map(data, ~ lm(y ~ x, data = .x)), glance = map(model, glance), coefficients = map(model, coef), corr = map(data, ~ cor(.x$x, .x$y)) ) all_models all_models %&gt;% select(dataset, coefficients) %&gt;% unnest_wider(coefficients) all_models %&gt;% select(dataset, corr) %&gt;% unnest_wider(corr) 6.2 Datenanalyse, Beispiel: Global Plastic Waste 6.3 Feedbackrunde "],
["references.html", "References", " References Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang und Richard Iannone. 2019. rmarkdown: Dynamic Documents for R. https://CRAN.R-project.org/package=rmarkdown. Chang, Winston. 2013. R Graphics Cookbook: Practical Recipes for Visualizing Data. 1 edition. Beijing Cambridge Farnham Köln Sebastopol Tokyo: O’Reilly Media. Fisher, R. A. und F. Yates. 1990. Statistical Methods, Experimental Design, and Scientific Inference: A Re-Issue of Statistical Methods for Research Workers, The Design of Experiments, and Statistical Methods and Scientific Inference. Hg. von J. H. Bennett. 1 edition. Oxford England ; New York: Oxford University Press. Grolemund, Garrett und Hadley Wickham. 2014. Hands-On Programming with R: Write Your Own Functions and Simulations. Sebastopol, CA: O’Reilly Media. https://rstudio-education.github.io/hopr/. Healy, Kieran. 2018. Data Visualization: A Practical Introduction. 1 edition. Princeton, NJ: Princeton University Press. https://socviz.co/index.html. Horst, Allison. 2019. R &amp; Stats Illustrations by @allison_horst. Contribute to Allisonhorst/Stats-Illustrations Development by Creating an Account on GitHub. August. https://github.com/allisonhorst/stats-illustrations (zugegriffen: 26. August 2019). Kim, Chester Ismay and Albert Y. 2019. Statistical Inference via Data Science. CRC Press. https://moderndive.com/ (zugegriffen: 25. August 2019). Matejka, Justin und George Fitzmaurice. 2017. Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics Through Simulated Annealing. In: Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, 1290–1294. CHI ’17. New York, NY, USA: ACM. doi:10.1145/3025453.3025912, http://doi.acm.org/10.1145/3025453.3025912 (zugegriffen: 27. August 2019). Motulsky, Harvey. 2017. Intuitive Biostatistics: A Nonmathematical Guide to Statistical Thinking. 4 edition. New York: Oxford University Press. R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Reinhart, Alex. 2015. Statistics Done Wrong: The Woefully Complete Guide. 1 edition. San Francisco: No Starch Press. Wickham, Hadley. 2015. R Packages: Organize, Test, Document, and Share Your Code. 1 edition. Sebastopol, CA: O’Reilly Media. http://r-pkgs.had.co.nz/. ---. 2019. Advanced R, Second Edition. 2 edition. Boca Raton: Chapman and Hall/CRC. https://adv-r.hadley.nz/. Wickham, Hadley und Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1 edition. Sebastopol, CA: O’Reilly Media. https://r4ds.had.co.nz/. Xie, Yihui. 2015. Dynamic Documents with R and knitr. 2nd Aufl. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. ---. 2019a. bookdown: Authoring Books and Technical Documents with R Markdown. https://CRAN.R-project.org/package=bookdown. ---. 2019b. knitr: A General-Purpose Package for Dynamic Report Generation in R. https://CRAN.R-project.org/package=knitr. "]
]
