---
output: html_document
editor_options: 
  chunk_output_type: inline
---
# Folgt {#day5}

```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

## Lösung Tag 4

## Korrelation und Regression

https://figshare.com/articles/Storks_and_human_babies_data/839299/1


```{r}
library(tidyverse)

storks <- read_csv("data/05_storks.csv")
storks
```

```{r}
ggplot(storks, aes(Storks, Birth)) +
  geom_point()
```


```{r}
ggplot(storks, aes(Storks, Birth)) +
  geom_point() +
  scale_x_continuous(trans = "log", breaks = scales::log_breaks()) +
  scale_y_continuous(trans = "log", breaks = scales::log_breaks())
```

```{r}
cor(storks$Storks, storks$Birth)
```

```{r}
cor(log(storks$Storks), log(storks$Birth))
```


```{r}
cor.test(storks$Storks, storks$Birth)
```

```{r}
cor(storks$Storks, storks$Birth, method = "spearman")
```

```{r}
model <- lm(storks$Birth ~ storks$Storks)
summary(model)
```

```{r}
cor(storks$Storks, storks$Birth)^2
```

```{r}
with(storks, {
  plot(Storks, Birth)
  abline(lm(Birth~Storks))
})
```

```{r}
with(storks, {
  plot(log(Storks), log(Birth))
  abline(lm(log(Birth)~log(Storks)))
})
```



## Log-Transformationen

## Tricks für viele Daten / Dateien

```{r}

```


## Non-Linear Regression

```{r}
Puromycin <- as_tibble(Puromycin)
Puromycin
```

```{r}
puro_treat <- Puromycin %>% 
  filter(state == "treated")
```

```{r}
plot(puro_treat$conc, puro_treat$rate)
```

```{r}
ggplot(puro_treat, aes(conc, rate)) + geom_point()
```

```{r}
michaelis_menten_fun <- function(conc, Vm, K) {
  (Vm * conc) / (K + conc)
}
```

```{r}
plot(puro_treat$conc, puro_treat$rate)
curve(michaelis_menten_fun(conc = x, Vm = 200, K = 0.06),
      add = TRUE,
      col = "red",
      from = 0, to = 1,
      n = 100)
```

```{r}
model <- nls(rate ~ michaelis_menten_fun(conc, Vm, K),
             start = list(Vm = 200, K = 0.1),
             data = puro_treat)
model
```

```{r}
coef(model)
```

```{r}
Vm_est <- coef(model)[1]
K_est  <- coef(model)[2]
```

```{r}
plot(puro_treat$conc, puro_treat$rate)
curve(michaelis_menten_fun(x, Vm = Vm_est, K = K_est), add = TRUE, col = "red")
```

```{r}
ggplot(puro_treat, aes(conc, rate)) +
  geom_point() +
  stat_function(fun = ~michaelis_menten_fun(conc = .x, Vm = Vm_est, K = K_est),
                col = "red")
```


```{r}
all_models <- Puromycin %>% 
  group_nest(state) %>% 
  mutate(model = map(data, ~nls(rate ~ michaelis_menten_fun(conc, Vm, K),
                                start = list(Vm = 200, K = 0.1),
                                data = .x))
  )
all_models
```

```{r}


```

```{r}

```

```{r}

```





```{r,echo=FALSE}
## linear regression on RNA decay data:
# RNA levels decrease after transcription was shut down with Actinomycin D:

# As a scatter plot:

# lm() produces a "linear model":


# from the model, we get the intercept and the slope of the regression line.
# The first coefficient is the intercept, the second the slope:


## correlation coefficients of IP data:
# How good is the correlation between biological replicates? 
# (= How reproducible was the experiment?)


# Should we log-transform the data?


# Get Pearson's R:


# How is the correlation between conditions compared to the correlation between 
# replicates? (= How much information is in the data?)


# adding points to a scatter plot


## Relationship between regression and correlation:
# R squared is symmetrical and the same as the square of Pearson`s R 

# If both standard deviations are identical, the slope of the regression line
# is the same as Pearson's R.
# We standardize the data by subtracting the mean and dividing by the standard deviation:

# Now, Pearson's R and the slope of the regression are identical:

# lm() is now symmetric:


## Testing correlation coefficients:
# How do we calculate a p-value for correlation coefficients?
# Let's simulate the behaviour of Pearson's R if there is NO CORRELATION:


# We break the relationship between a and b by shuffling the values randomly:

# Are correlation coefficients always normally distributed, or is there a connection to sample size?
# Let's decrease sample size (n):

# Pearson's R follows something like a t-distribution, 
# after this transformation:


## Exercises:
# 1a. Open example6.csv and plot y against x.

# b. Add a regression line and calculate Pearson‘s
# correlation coefficient.

# c. Compare to the Spearman correlation coefficient.

# d. Remove the last data point of x and y. How does this
# change the result of a - c?

# 2. File example7.csv contains the number of stork pairs and the
# birth rate (x 10^3 / year) of 17 countries in 1990. (Source: Teaching Statistics, Volume 22 ).
# 2a. Produce a scatter plot and add a regression line. 
# 2b. Calculate Pearson`s correlation coefficient and Spearman`s correlation coefficient 
# for the data as they are and after log-transformation. 
# How does the log-transformation affect the
# correlation coefficients?
# 2c. Perform a permutation test on Pearson's R of the data 
# (without log-transformation).


# 3. The file example8.csv contains transcriptome-wide mRNA half life measurements and
# four different properties of the mRNAs: Information about the AREScore, 5‘UTR, ORF
# and 3‘UTR length.
# Which of these properties has a significant correlation with half life?

# a. Plot the different mRNA properties against mRNA half-lives. Calculate the respective
# correlation coefficients. When would you choose Pearson`s and when would you choose
# Spearman`s method? When would you log-transform the data? Check for normal or log-
#  normal distribution!

# b. Plot the half life against the property that shows the best correlation with half-life and
# add a regression line to your plot.

# c. Two weeks ago, you were able to show a significant positive association between
# AREScore and conserved as well as not-conserved CDEs. Correlate AREScore and 3‘UTR
# length. In addition, make a boxplot showing the 3‘UTR lengths of genes with no CDE, a
# CDE in the ORF, a CDE that is not-conserved and a conserved CDE. What would you
# conclude?


```






